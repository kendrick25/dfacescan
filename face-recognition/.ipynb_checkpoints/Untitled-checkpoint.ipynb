{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ea2f73-cdac-4e5d-8ed4-bd7aee053b19",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1d8c20-148a-42ab-a8f3-752f18f89da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.15 | packaged by Anaconda, Inc. | (main, Oct  3 2024, 07:22:19) [MSC v.1929 64 bit (AMD64)]\n",
      "1.26.4\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from deepface import DeepFace\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "print(sys.version)\n",
    "print(np.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da536587-a852-4c30-b2d0-a0be0210839c",
   "metadata": {},
   "source": [
    "### Funciones para extraer faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94aa04a4-3338-4c8c-81e1-08feb2edc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pixels_from_image(image: np.ndarray, facial_area: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extrae el área de la imagen basado en las coordenadas de 'facial_area' y devuelve\n",
    "    el recorte como un array en formato float64.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Imagen original cargada con OpenCV en formato uint8 BGR.\n",
    "        facial_area (dict): Diccionario con las coordenadas 'x', 'y', 'w', 'h'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: El recorte del rostro en formato float64 y en el rango [0, 1].\n",
    "    \"\"\"\n",
    "    x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "    \n",
    "    # Ajustar las coordenadas para evitar índices negativos\n",
    "    x = max(0, x)\n",
    "    y = max(0, y)\n",
    "    \n",
    "    # Recortar la imagen en la región del área facial\n",
    "    face_crop = image[y:y+h, x:x+w]\n",
    "\n",
    "    # Convertir el recorte a float64 y escalar a [0, 1]\n",
    "    face_crop_float64 = face_crop.astype(np.float64) / 255.0\n",
    "    \n",
    "    return face_crop_float64\n",
    "\n",
    "def extract_and_expand_faces(img_path: str, margin_ratio: float = 0.0) -> list:\n",
    "    \"\"\"\n",
    "    Extrae rostros de una imagen y expande los bounding boxes según el margin_ratio.\n",
    "    Args:\n",
    "        img_path (str): Ruta de la imagen para procesar.\n",
    "        margin_ratio (float): Proporción de expansión del bounding box. \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: Lista de rostros detectados con bounding boxes ajustados.\n",
    "    \"\"\"\n",
    "    # Cargar la imagen\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    # Detectar rostros con DeepFace usando el modelo RetinaFace\n",
    "    faces = DeepFace.extract_faces(\n",
    "        img_path=image,\n",
    "        detector_backend='retinaface',\n",
    "        enforce_detection=False\n",
    "    )\n",
    "\n",
    "    # Expande los bounding boxes\n",
    "    for face_data in faces:\n",
    "        facial_area = face_data['facial_area']\n",
    "        \n",
    "        # Obtener coordenadas originales\n",
    "        x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "        \n",
    "        # Calcular margen adicional basado en el ratio\n",
    "        margin_x = int(w * margin_ratio)\n",
    "        margin_y = int(h * margin_ratio)\n",
    "        \n",
    "        # Expande el bounding box con el margen calculado\n",
    "        facial_area['x'] = x - margin_x\n",
    "        facial_area['y'] = y - margin_y\n",
    "        facial_area['w'] = w + 2 * margin_x\n",
    "        facial_area['h'] = h + 2 * margin_y\n",
    "\n",
    "    for __face__ in faces:\n",
    "        __face__['face'] = extract_pixels_from_image(image, __face__['facial_area'])\n",
    "    \n",
    "    return faces\n",
    "\n",
    "def get_img_array_uint8(arraydata):\n",
    "    img = arraydata\n",
    "    #B, G, R = img.T\n",
    "    #__bgr_img = np.array((B, G, R)).T\n",
    "    #bgr_img = (__bgr_img*255).astype(np.uint8)\n",
    "    bgr_img = (img*255).astype(np.uint8)\n",
    "    return bgr_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58301c-523e-4444-be26-3d4446572aa2",
   "metadata": {},
   "source": [
    "### Diccionario de peronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21500ead-283f-4b69-aa91-887367605f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'bernal-jaime', 'images': ['img-00.png', 'img-01.png', 'img-02.png', 'img-03.png', 'img-04.png'], 'images_paths': ['./data/classroom/ref/bernal-jaime/img-00.png', './data/classroom/ref/bernal-jaime/img-01.png', './data/classroom/ref/bernal-jaime/img-02.png', './data/classroom/ref/bernal-jaime/img-03.png', './data/classroom/ref/bernal-jaime/img-04.png']}\n"
     ]
    }
   ],
   "source": [
    "def get_people_list(path_people_ref:str)->list:\n",
    "    # Ruta del directorio de referencia\n",
    "    ref_directory = path_people_ref\n",
    "\n",
    "    # Lista para almacenar los diccionarios de las personas\n",
    "    list_dict = []\n",
    "\n",
    "    # Recorrer cada subdirectorio en el directorio de referencia\n",
    "    for person_name in os.listdir(ref_directory):\n",
    "        person_path = os.path.join(ref_directory, person_name)\n",
    "    \n",
    "        # Verificar si es un directorio (nombre de persona)\n",
    "        if os.path.isdir(person_path):\n",
    "            # Listar las imágenes dentro del subdirectorio de la persona\n",
    "            images = [img for img in os.listdir(person_path) if img.endswith(\".png\")]\n",
    "            \n",
    "            # Generar rutas completas con \"/\" como separador\n",
    "            image_paths = [os.path.join(person_path, img).replace(\"\\\\\", \"/\") for img in images]\n",
    "        \n",
    "            # Crear el diccionario para esta persona\n",
    "            person_dict = {\n",
    "                \"name\": person_name,\n",
    "                \"images\": images,\n",
    "                \"images_paths\": image_paths\n",
    "            }\n",
    "        \n",
    "            # Añadir a la lista\n",
    "            list_dict.append(person_dict)\n",
    "    \n",
    "    return list_dict\n",
    "\n",
    "# Generar la lista de personas\n",
    "list_dict_people = get_people_list(\"./data/classroom/ref/\")\n",
    "\n",
    "# Ejemplo de acceso y salida\n",
    "print(list_dict_people[2])  # Diccionario del primer elemento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb7b8a-60b0-4b0f-a78b-922632096a6e",
   "metadata": {},
   "source": [
    "### Iteraciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb3741f3-1b18-4e21-b5f1-759e15843844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_person(\n",
    "    analyze_faces: list, \n",
    "    ref_face: list, \n",
    "    metric_model_name: str = \"Facenet512\",\n",
    "    metric_distance: str = \"cosine\",\n",
    "    metric_threshold: float = 0.44\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Reconoce a una persona comparando los rostros detectados en la imagen de análisis\n",
    "    con el rostro de referencia, utilizando DeepFace.\n",
    "\n",
    "    Args:\n",
    "        analyze_faces (list): Lista de rostros detectados en la imagen de análisis.\n",
    "        ref_face (list): Lista con el rostro objetivo (referencia).\n",
    "        analyze_margin_ratio (float): Margen de expansión para los rostros analizados.\n",
    "        ref_margin_ratio (float): Margen de expansión para el rostro de referencia.\n",
    "        metric_model_name (str): Modelo a utilizar para la comparación.\n",
    "        metric_distance (str): Métrica de distancia para la comparación.\n",
    "        metric_threshold (float): Umbral para verificar si los rostros coinciden.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de diccionarios con información de los resultados para cada rostro.\n",
    "    \"\"\"\n",
    "    if not ref_face or len(ref_face) == 0:\n",
    "        print(\"Error: The reference face is empty.\")\n",
    "        return []\n",
    "    \n",
    "    # Obtener el rostro de referencia\n",
    "    target_face_data = ref_face[0]\n",
    "    target_face = target_face_data['face']\n",
    "    array_bgr_target_face = (target_face * 255.0).astype(np.uint8)\n",
    "\n",
    "    results = []\n",
    "    total_faces = len(analyze_faces)\n",
    "\n",
    "    # Iterar sobre cada rostro detectado en la imagen de análisis\n",
    "    for i, face_data in enumerate(analyze_faces):\n",
    "        # Progreso de la comparación\n",
    "        loading = f\"{(i / (total_faces - 1)) * 100:.3f}%\"\n",
    "        print(\"\\r\" + loading, end=\"\")\n",
    "\n",
    "        facial_area = face_data['facial_area']  # Bounding box\n",
    "        face = face_data['face']\n",
    "        array_bgr_face = (face * 255.0).astype(np.uint8)\n",
    "\n",
    "        # Comparar el rostro detectado con el rostro de referencia\n",
    "        result = DeepFace.verify(\n",
    "            img1_path=array_bgr_target_face,\n",
    "            img2_path=array_bgr_face,\n",
    "            detector_backend=\"skip\",\n",
    "            model_name=metric_model_name,\n",
    "            distance_metric=metric_distance,\n",
    "            threshold=metric_threshold,\n",
    "            enforce_detection=False\n",
    "        )\n",
    "\n",
    "        # Almacenar los resultados\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"distance\": result['distance'],\n",
    "            \"threshold\": result['threshold'],\n",
    "            \"verified\": result['verified'],\n",
    "            \"facial_area\": facial_area\n",
    "        })\n",
    "\n",
    "    # Ordenar resultados por distancia\n",
    "    results.sort(key=lambda x: x['distance'])\n",
    "\n",
    "    # Imprimir resultados ordenados\n",
    "    #for res in results:\n",
    "        #print(f\"Index: {res['index']} Distance: {res['distance']:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "253146e6-4b51-4ae3-ab75-86a72187bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_verified_faces(image, result_list_dict, name_person=\"target\"):\n",
    "    \"\"\"\n",
    "    Dibuja los recuadros en la imagen basados en los resultados de la verificación.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): Imagen original en formato BGR.\n",
    "        result_list_dict (list): Lista de resultados de verificación.\n",
    "        name_person (str): Nombre del objetivo reconocido.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Imagen con los recuadros dibujados.\n",
    "    \"\"\"\n",
    "    for result in result_list_dict:\n",
    "        # Obtener coordenadas del área facial\n",
    "        facial_area = result['facial_area']\n",
    "        x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "\n",
    "        # Verificar si el rostro es reconocido (verified=True)\n",
    "\n",
    "        if result['verified']:\n",
    "            color = (0, 255, 0)  # Verde\n",
    "            confidence = 1.0 - result['distance']\n",
    "            label = f\"{name_person} ({confidence:.2f}%)\"\n",
    "            # Dibujar el recuadro y la etiqueta\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95e048-7dfe-4f90-9936-cfd62ed28886",
   "metadata": {},
   "source": [
    "### Funcion para colisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "611fdb47-6137-41db-a4ea-98d84d8e0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_face_collisions(faces_data, target_face_data, metric_threshold=0.395):\n",
    "    \"\"\"\n",
    "    Maneja las colisiones de coincidencias entre rostros, realiza comparaciones con dos umbrales y\n",
    "    maneja la selección de la mejor coincidencia.\n",
    "\n",
    "    Args:\n",
    "        faces_data (list): Lista de rostros detectados en la imagen de análisis.\n",
    "        target_face_data (list): Lista con los rostros de referencia para comparación.\n",
    "        metric_threshold (float): Umbral para la coincidencia de rostros.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de resultados ordenados con los rostros reconocidos y sus índices.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    best_match_results = []\n",
    "\n",
    "    # Primer recorrido con el umbral inicial\n",
    "    first_pass_results = recognize_person(\n",
    "        faces_data, target_face_data, metric_model_name, metric_distance, metric_threshold\n",
    "    )\n",
    "    \n",
    "    for result in first_pass_results:\n",
    "        if result['verified']:\n",
    "            # Si la persona está verificada, añadimos a la lista\n",
    "            results.append(result)\n",
    "\n",
    "    # Si encontramos resultados verificados, realizamos otro recorrido con un umbral más alto\n",
    "    if results:\n",
    "        second_pass_results = recognize_person(\n",
    "            faces_data, target_face_data, metric_model_name, metric_distance, metric_threshold\n",
    "        )\n",
    "\n",
    "        # Comparamos las precisión entre los resultados obtenidos\n",
    "        for first_result, second_result in zip(first_pass_results, second_pass_results):\n",
    "            if first_result['verified'] and second_result['verified']:\n",
    "                # Si ambos resultados están verificados, comparar la distancia\n",
    "                if second_result['distance'] < first_result['distance']:\n",
    "                    # Si la segunda imagen tiene una mejor coincidencia, actualizar la referencia\n",
    "                    best_match_results.append(second_result)\n",
    "                else:\n",
    "                    best_match_results.append(first_result)\n",
    "            else:\n",
    "                # Si alguno no está verificado, lo descartamos\n",
    "                continue\n",
    "    else:\n",
    "        # Si no hubo verificación en el primer paso, usamos un margen más amplio y otro rostro de referencia\n",
    "        print(\"No verified face found, trying with adjusted margin...\")\n",
    "\n",
    "        for i, ref_data in enumerate(target_face_data):\n",
    "            if i > 0:  # Ya hemos procesado el primer rostro, usamos el siguiente como referencia\n",
    "                print(f\"Trying reference face {i+1} with adjusted margin.\")\n",
    "                second_face_data = extract_and_expand_faces(ref_data[\"images_paths\"][0], margin_ratio=0.2)\n",
    "                collision_results = recognize_person(faces_data, second_face_data, metric_model_name, metric_distance, metric_threshold)\n",
    "\n",
    "                if collision_results:\n",
    "                    for res in collision_results:\n",
    "                        # Si la precisión es alta, consideramos que hemos encontrado a la persona\n",
    "                        if res['verified']:\n",
    "                            best_match_results.append(res)\n",
    "                            break\n",
    "\n",
    "    # Si encontramos alguna coincidencia en los resultados, actualizamos las coincidencias de rostros\n",
    "    if best_match_results:\n",
    "        # Filtrar por los resultados más cercanos, es decir, con menor distancia\n",
    "        best_match_results.sort(key=lambda x: x['distance'])\n",
    "        return best_match_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a7675-7d0b-43e2-b136-ead2bff53554",
   "metadata": {},
   "source": [
    "### Codigo Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29194c99-3179-498c-a9cb-a33d88216557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:35\u001b[1;36m\u001b[0m\n\u001b[1;33m    result_list_dict = handle_face_collisions(faces_data, target_face_data, metric_threshold=0.395)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Define the metrics and models\n",
    "metrics = [\"cosine\", \"euclidean\", \"euclidean_l2\"]\n",
    "models = [\"Facenet\", \"Facenet512\", \"VGG-Face\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\", \"SFace\", \"GhostFaceNet\"]\n",
    "\n",
    "# Cargar la imagen a analizar\n",
    "path_analyze_image = \"./data/classroom/rawdata/s2/recording_2024-11-13_10-45-09/frame-237.jpg\"\n",
    "image = cv2.imread(path_analyze_image)\n",
    "\n",
    "# Extraer los rostros de la imagen a analizar\n",
    "print(\"Extracting faces...\")\n",
    "faces_data = extract_and_expand_faces(path_analyze_image, 0.2)\n",
    "\n",
    "# Inicializar la variable para la imagen con los rostros verificados\n",
    "image_with_verified_face = image\n",
    "\n",
    "# Cargar las personas de referencia\n",
    "print(\"Loading reference people...\")\n",
    "list_dict_people = get_people_list(\"./data/classroom/ref-standard/\")\n",
    "\n",
    "# Iterar sobre las personas de referencia\n",
    "index = 0\n",
    "for dict_person in list_dict_people:\n",
    "    print(f\"Person: {index + 1}/{len(list_dict_people)}\")\n",
    "    \n",
    "    person_name = dict_person[\"name\"]\n",
    "    path_ref_img = dict_person[\"images_paths\"][0]\n",
    "    print(f\"Analyzing the person {person_name}\")\n",
    "    \n",
    "    # Extraer el rostro objetivo (face_target) de la imagen de referencia\n",
    "    print(\"Extracting reference face...\")\n",
    "    target_face_data = extract_and_expand_faces(path_ref_img, 0.18)\n",
    "    \n",
    "    # Comparar los rostros usando la función de manejo de colisiones\n",
    "    print(\"Handling face collisions...\")\n",
    "   result_list_dict = handle_face_collisions(faces_data, target_face_data, metric_threshold=0.395)\n",
    "\n",
    "    \n",
    "    print(\"\\nDrawing box...\\n\")\n",
    "    image_with_verified_face = draw_verified_faces(image_with_verified_face, result_list_dict, name_person=person_name)\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "# Convertir BGR a RGB para mostrar con Matplotlib\n",
    "image_rgb = cv2.cvtColor(image_with_verified_face, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Mostrar la imagen con Matplotlib\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('on')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82415bac-f91e-46a0-a8ad-932d471e326c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
