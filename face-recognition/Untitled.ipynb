{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ea2f73-cdac-4e5d-8ed4-bd7aee053b19",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1d8c20-148a-42ab-a8f3-752f18f89da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.15 | packaged by Anaconda, Inc. | (main, Oct  3 2024, 07:22:19) [MSC v.1929 64 bit (AMD64)]\n",
      "1.26.4\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time  # Solo para simular tiempo de procesamiento, puedes quitar esto\n",
    "import os\n",
    "from deepface import DeepFace\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "print(sys.version)\n",
    "print(np.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da536587-a852-4c30-b2d0-a0be0210839c",
   "metadata": {},
   "source": [
    "### Funciones para extraer faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94aa04a4-3338-4c8c-81e1-08feb2edc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pixels_from_image(image: np.ndarray, facial_area: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extrae el área de la imagen basado en las coordenadas de 'facial_area' y devuelve\n",
    "    el recorte como un array en formato float64.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Imagen original cargada con OpenCV en formato uint8 BGR.\n",
    "        facial_area (dict): Diccionario con las coordenadas 'x', 'y', 'w', 'h'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: El recorte del rostro en formato float64 y en el rango [0, 1].\n",
    "    \"\"\"\n",
    "    x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "    \n",
    "    # Ajustar las coordenadas para evitar índices negativos\n",
    "    x = max(0, x)\n",
    "    y = max(0, y)\n",
    "    \n",
    "    # Recortar la imagen en la región del área facial\n",
    "    face_crop = image[y:y+h, x:x+w]\n",
    "\n",
    "    # Convertir el recorte a float64 y escalar a [0, 1]\n",
    "    face_crop_float64 = face_crop.astype(np.float64) / 255.0\n",
    "    \n",
    "    return face_crop_float64\n",
    "\n",
    "def extract_and_expand_faces(img_path: str, margin_ratio: float = 0.0) -> list:\n",
    "    \"\"\"\n",
    "    Extrae rostros de una imagen y expande los bounding boxes según el margin_ratio.\n",
    "    Args:\n",
    "        img_path (str): Ruta de la imagen para procesar.\n",
    "        margin_ratio (float): Proporción de expansión del bounding box. \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: Lista de rostros detectados con bounding boxes ajustados.\n",
    "    \"\"\"\n",
    "    # Cargar la imagen\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    # Detectar rostros con DeepFace usando el modelo RetinaFace\n",
    "    faces = DeepFace.extract_faces(\n",
    "        img_path=image,\n",
    "        detector_backend='retinaface',\n",
    "        enforce_detection=False\n",
    "    )\n",
    "\n",
    "    # Expande los bounding boxes\n",
    "    for face_data in faces:\n",
    "        facial_area = face_data['facial_area']\n",
    "        \n",
    "        # Obtener coordenadas originales\n",
    "        x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "        \n",
    "        # Calcular margen adicional basado en el ratio\n",
    "        margin_x = int(w * margin_ratio)\n",
    "        margin_y = int(h * margin_ratio)\n",
    "        \n",
    "        # Expande el bounding box con el margen calculado\n",
    "        facial_area['x'] = x - margin_x\n",
    "        facial_area['y'] = y - margin_y\n",
    "        facial_area['w'] = w + 2 * margin_x\n",
    "        facial_area['h'] = h + 2 * margin_y\n",
    "\n",
    "    for __face__ in faces:\n",
    "        __face__['face'] = extract_pixels_from_image(image, __face__['facial_area'])\n",
    "    \n",
    "    return faces\n",
    "\n",
    "def get_img_array_uint8(arraydata):\n",
    "    img = arraydata\n",
    "    #B, G, R = img.T\n",
    "    #__bgr_img = np.array((B, G, R)).T\n",
    "    #bgr_img = (__bgr_img*255).astype(np.uint8)\n",
    "    bgr_img = (img*255).astype(np.uint8)\n",
    "    return bgr_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58301c-523e-4444-be26-3d4446572aa2",
   "metadata": {},
   "source": [
    "### Diccionario de peronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21500ead-283f-4b69-aa91-887367605f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'bernal-jaime', 'images': ['img-00.png', 'img-01.png', 'img-02.png', 'img-03.png', 'img-04.png'], 'images_paths': ['./data/classroom/ref/bernal-jaime/img-00.png', './data/classroom/ref/bernal-jaime/img-01.png', './data/classroom/ref/bernal-jaime/img-02.png', './data/classroom/ref/bernal-jaime/img-03.png', './data/classroom/ref/bernal-jaime/img-04.png']}\n"
     ]
    }
   ],
   "source": [
    "def get_people_list(path_people_ref:str)->list:\n",
    "    # Ruta del directorio de referencia\n",
    "    ref_directory = path_people_ref\n",
    "\n",
    "    # Lista para almacenar los diccionarios de las personas\n",
    "    list_dict = []\n",
    "\n",
    "    # Recorrer cada subdirectorio en el directorio de referencia\n",
    "    for person_name in os.listdir(ref_directory):\n",
    "        person_path = os.path.join(ref_directory, person_name)\n",
    "    \n",
    "        # Verificar si es un directorio (nombre de persona)\n",
    "        if os.path.isdir(person_path):\n",
    "            # Listar las imágenes dentro del subdirectorio de la persona\n",
    "            images = [img for img in os.listdir(person_path) if img.endswith(\".png\")]\n",
    "            \n",
    "            # Generar rutas completas con \"/\" como separador\n",
    "            image_paths = [os.path.join(person_path, img).replace(\"\\\\\", \"/\") for img in images]\n",
    "        \n",
    "            # Crear el diccionario para esta persona\n",
    "            person_dict = {\n",
    "                \"name\": person_name,\n",
    "                \"images\": images,\n",
    "                \"images_paths\": image_paths\n",
    "            }\n",
    "        \n",
    "            # Añadir a la lista\n",
    "            list_dict.append(person_dict)\n",
    "    \n",
    "    return list_dict\n",
    "\n",
    "# Generar la lista de personas\n",
    "list_dict_people = get_people_list(\"./data/classroom/ref/\")\n",
    "\n",
    "# Ejemplo de acceso y salida\n",
    "print(list_dict_people[2])  # Diccionario del primer elemento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb7b8a-60b0-4b0f-a78b-922632096a6e",
   "metadata": {},
   "source": [
    "### Iteraciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3741f3-1b18-4e21-b5f1-759e15843844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_person(\n",
    "    analyze_faces: list, \n",
    "    ref_face: list, \n",
    "    metric_model_name: str = \"Facenet512\",\n",
    "    metric_distance: str = \"cosine\",\n",
    "    metric_threshold: float = 0.44\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Reconoce a una persona comparando los rostros detectados en la imagen de análisis\n",
    "    con el rostro de referencia, utilizando DeepFace.\n",
    "\n",
    "    Args:\n",
    "        analyze_faces (list): Lista de rostros detectados en la imagen de análisis.\n",
    "        ref_face (list): Lista con el rostro objetivo (referencia).\n",
    "        analyze_margin_ratio (float): Margen de expansión para los rostros analizados.\n",
    "        ref_margin_ratio (float): Margen de expansión para el rostro de referencia.\n",
    "        metric_model_name (str): Modelo a utilizar para la comparación.\n",
    "        metric_distance (str): Métrica de distancia para la comparación.\n",
    "        metric_threshold (float): Umbral para verificar si los rostros coinciden.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de diccionarios con información de los resultados para cada rostro.\n",
    "    \"\"\"\n",
    "    if not ref_face or len(ref_face) == 0:\n",
    "        print(\"Error: The reference face is empty.\")\n",
    "        return []\n",
    "    \n",
    "    # Obtener el rostro de referencia\n",
    "    target_face_data = ref_face[0]\n",
    "    target_face = target_face_data['face']\n",
    "    array_bgr_target_face = (target_face * 255.0).astype(np.uint8)\n",
    "\n",
    "    results = []\n",
    "    total_faces = len(analyze_faces)\n",
    "\n",
    "    # Iterar sobre cada rostro detectado en la imagen de análisis\n",
    "    for i, face_data in enumerate(analyze_faces):\n",
    "        # Progreso de la comparación\n",
    "        loading = f\"{(i / (total_faces - 1)) * 100:.3f}%\"\n",
    "        print(\"\\r\" + loading, end=\"\")\n",
    "\n",
    "        facial_area = face_data['facial_area']  # Bounding box\n",
    "        face = face_data['face']\n",
    "        array_bgr_face = (face * 255.0).astype(np.uint8)\n",
    "\n",
    "        # Comparar el rostro detectado con el rostro de referencia\n",
    "        result = DeepFace.verify(\n",
    "            img1_path=array_bgr_target_face,\n",
    "            img2_path=array_bgr_face,\n",
    "            detector_backend=\"skip\",\n",
    "            model_name=metric_model_name,\n",
    "            distance_metric=metric_distance,\n",
    "            threshold=metric_threshold,\n",
    "            enforce_detection=False\n",
    "        )\n",
    "\n",
    "        # Almacenar los resultados\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"distance\": result['distance'],\n",
    "            \"threshold\": result['threshold'],\n",
    "            \"verified\": result['verified'],\n",
    "            \"facial_area\": facial_area\n",
    "        })\n",
    "\n",
    "    # Ordenar resultados por distancia\n",
    "    results.sort(key=lambda x: x['distance'])\n",
    "\n",
    "    # Imprimir resultados ordenados\n",
    "    #for res in results:\n",
    "        #print(f\"Index: {res['index']} Distance: {res['distance']:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253146e6-4b51-4ae3-ab75-86a72187bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_verified_faces(image, result_list_dict, name_person=\"target\"):\n",
    "    \"\"\"\n",
    "    Dibuja los recuadros en la imagen basados en los resultados de la verificación.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): Imagen original en formato BGR.\n",
    "        result_list_dict (list): Lista de resultados de verificación.\n",
    "        name_person (str): Nombre del objetivo reconocido.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Imagen con los recuadros dibujados.\n",
    "    \"\"\"\n",
    "    for result in result_list_dict:\n",
    "        # Obtener coordenadas del área facial\n",
    "        facial_area = result['facial_area']\n",
    "        x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "\n",
    "        # Verificar si el rostro es reconocido (verified=True)\n",
    "\n",
    "        if result['verified']:\n",
    "            color = (0, 255, 0)  # Verde\n",
    "            confidence = 1.0 - result['distance']\n",
    "            label = f\"{name_person} ({confidence:.2f}%)\"\n",
    "            # Dibujar el recuadro y la etiqueta\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95e048-7dfe-4f90-9936-cfd62ed28886",
   "metadata": {},
   "source": [
    "### Funcion para colisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c8ce4f-46ce-4d10-8caa-40b4a70e1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_face_collisions(\n",
    "    analyze_faces, ref_face_list, person_name, metric_model_name=\"Facenet512\", \n",
    "    metric_distance=\"cosine\", initial_threshold=0.4, final_threshold=0.465, max_ref_faces=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Gestiona las colisiones y coincidencias entre rostros detectados y rostros de referencia.\n",
    "\n",
    "    Args:\n",
    "        analyze_faces (list): Lista de rostros detectados en la imagen de análisis.\n",
    "        ref_face_list (list): Lista de rostros de referencia (puede incluir múltiples imágenes).\n",
    "        person_name (str): Nombre de la persona de referencia.\n",
    "        metric_model_name (str): Modelo para la comparación.\n",
    "        metric_distance (str): Métrica de distancia para la comparación.\n",
    "        initial_threshold (float): Umbral inicial para la verificación.\n",
    "        final_threshold (float): Umbral final para la verificación.\n",
    "        max_ref_faces (int): Número máximo de rostros de referencia a comparar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de diccionarios con los rostros verificados y su precisión.\n",
    "    \"\"\"\n",
    "    # Limitar la cantidad de rostros de referencia\n",
    "    ref_face_list = ref_face_list[:max_ref_faces]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    total_ref_faces = len(ref_face_list)\n",
    "    total_analyze_faces = len(analyze_faces)\n",
    "    total_comparisons = total_ref_faces * total_analyze_faces\n",
    "\n",
    "    comparison_counter = 0\n",
    "\n",
    "    # Iterar sobre las imágenes de referencia\n",
    "    for ref_face_data in ref_face_list:\n",
    "        target_face = ref_face_data['face']\n",
    "        array_bgr_target_face = (target_face * 255.0).astype(np.uint8)\n",
    "\n",
    "        for face_data in analyze_faces:\n",
    "            face = face_data['face']\n",
    "            array_bgr_face = (face * 255.0).astype(np.uint8)\n",
    "\n",
    "            # Primera comparación con umbral bajo\n",
    "            initial_result = DeepFace.verify(\n",
    "                img1_path=array_bgr_target_face,\n",
    "                img2_path=array_bgr_face,\n",
    "                detector_backend=\"skip\",\n",
    "                model_name=metric_model_name,\n",
    "                distance_metric=metric_distance,\n",
    "                threshold=initial_threshold,\n",
    "                enforce_detection=False\n",
    "            )\n",
    "            \n",
    "            if initial_result['verified']:\n",
    "                # Segunda comparación con umbral más alto para confirmar\n",
    "                final_result = DeepFace.verify(\n",
    "                    img1_path=array_bgr_target_face,\n",
    "                    img2_path=array_bgr_face,\n",
    "                    detector_backend=\"skip\",\n",
    "                    model_name=metric_model_name,\n",
    "                    distance_metric=metric_distance,\n",
    "                    threshold=final_threshold,\n",
    "                    enforce_detection=False\n",
    "                )\n",
    "\n",
    "                # Añadir el resultado a la lista si está verificado\n",
    "                if final_result['verified']:\n",
    "                    confidence = 1.0 - final_result['distance']\n",
    "                    results.append({\n",
    "                        \"name\": person_name,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"distance\": final_result['distance'],\n",
    "                        \"facial_area\": face_data['facial_area'],\n",
    "                        \"verified\": True\n",
    "                    })\n",
    "            \n",
    "            # Actualización del progreso\n",
    "            comparison_counter += 1\n",
    "            progress = (comparison_counter / total_comparisons) * 100\n",
    "            sys.stdout.write(f\"\\rProcessing... {comparison_counter}/{total_comparisons} comparisons ({progress:.2f}%)\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    # Filtrar y resolver colisiones por precisión\n",
    "    if len(results) > 1:\n",
    "        results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        best_result = results[0]\n",
    "        results = [best_result]\n",
    "\n",
    "    print(f\"\\nFinished processing {person_name}. {len(results)} result(s) found.\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a7675-7d0b-43e2-b136-ead2bff53554",
   "metadata": {},
   "source": [
    "### Codigo Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd930b30-ebc6-4184-aaa0-a38bd4a6e755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting faces...\n",
      "Comparing faces...\n",
      "Person: 1/20\n",
      "Handling collisions for aparicio-azner...\n",
      "Processing... 24/24 comparisons (100.00%)\n",
      "Finished processing aparicio-azner. 1 result(s) found.\n",
      "Drawing boxes for verified faces...\n",
      "..................................................\n",
      "Person: 2/20\n",
      "Handling collisions for ayala-javier...\n",
      "Processing... 8/8 comparisons (100.00%)\n",
      "Finished processing ayala-javier. 0 result(s) found.\n",
      "Drawing boxes for verified faces...\n",
      "..................................................\n",
      "Person: 3/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m ref_faces_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_img_path \u001b[38;5;129;01min\u001b[39;00m ref_image_paths:\n\u001b[1;32m---> 33\u001b[0m     ref_faces_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_and_expand_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_img_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.18\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Limitar el número de rostros de referencia a max_ref_faces\u001b[39;00m\n\u001b[0;32m     36\u001b[0m limited_ref_faces \u001b[38;5;241m=\u001b[39m ref_faces_list[:max_ref_faces]  \u001b[38;5;66;03m# Solo tomamos los primeros max_ref_faces\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m, in \u001b[0;36mextract_and_expand_faces\u001b[1;34m(img_path, margin_ratio)\u001b[0m\n\u001b[0;32m     37\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Detectar rostros con DeepFace usando el modelo RetinaFace\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretinaface\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Expande los bounding boxes\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face_data \u001b[38;5;129;01min\u001b[39;00m faces:\n",
      "File \u001b[1;32m~\\.conda\\envs\\flow\\lib\\site-packages\\deepface\\DeepFace.py:549\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img_path, detector_backend, enforce_detection, align, expand_percentage, grayscale, color_face, normalize_face, anti_spoofing)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_faces\u001b[39m(\n\u001b[0;32m    490\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    491\u001b[0m     detector_backend: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopencv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m     anti_spoofing: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    499\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    500\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03m    Extract faces from a given image\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m            just available in the result only if anti_spoofing is set to True in input arguments.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrayscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_face\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_face\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_face\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_face\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\flow\\lib\\site-packages\\deepface\\modules\\detection.py:95\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img_path, detector_backend, enforce_detection, align, expand_percentage, grayscale, color_face, normalize_face, anti_spoofing)\u001b[0m\n\u001b[0;32m     93\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m [DetectedFace(img\u001b[38;5;241m=\u001b[39mimg, facial_area\u001b[38;5;241m=\u001b[39mbase_region, confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# in case of no face found\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(face_objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m enforce_detection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\flow\\lib\\site-packages\\deepface\\modules\\detection.py:234\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(detector_backend, img, align, expand_percentage)\u001b[0m\n\u001b[0;32m    223\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcopyMakeBorder(\n\u001b[0;32m    224\u001b[0m         img,\n\u001b[0;32m    225\u001b[0m         height_border,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m         value\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# Color of the border (black)\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# find facial areas of given image\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m facial_areas \u001b[38;5;241m=\u001b[39m \u001b[43mface_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m facial_area \u001b[38;5;129;01min\u001b[39;00m facial_areas:\n",
      "File \u001b[1;32m~\\.conda\\envs\\flow\\lib\\site-packages\\deepface\\models\\face_detection\\RetinaFace.py:23\u001b[0m, in \u001b[0;36mRetinaFaceClient.detect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mDetect and align face with retinaface\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    results (List[FacialAreaRegion]): A list of FacialAreaRegion objects\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m resp \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 23\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\.conda\\envs\\flow\\lib\\site-packages\\retinaface\\RetinaFace.py:122\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[0;32m    120\u001b[0m scores_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    121\u001b[0m landmarks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 122\u001b[0m im_tensor, im_info, im_scale \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_upscaling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m net_out \u001b[38;5;241m=\u001b[39m model(im_tensor)\n\u001b[0;32m    124\u001b[0m net_out \u001b[38;5;241m=\u001b[39m [elt\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m net_out]\n",
      "File \u001b[1;32m~\\.conda\\envs\\flow\\lib\\site-packages\\retinaface\\commons\\preprocess.py:137\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(img, allow_upscaling)\u001b[0m\n\u001b[0;32m    134\u001b[0m scales \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1980\u001b[39m]\n\u001b[0;32m    136\u001b[0m img, im_scale \u001b[38;5;241m=\u001b[39m resize_image(img, scales, allow_upscaling)\n\u001b[1;32m--> 137\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m im_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Make image scaling + BGR2RGB conversion + transpose (N,H,W,C) to (N,C,H,W)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = [\"cosine\", \"euclidean\", \"euclidean_l2\"]\n",
    "models = [\"Facenet\", \"Facenet512\", \"VGG-Face\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\", \"SFace\", \"GhostFaceNet\"]\n",
    "\n",
    "# Cargar la imagen a analizar\n",
    "path_analyze_image = \"./data/classroom/rawdata/s2/recording_2024-11-13_10-45-09/frame-138.jpg\"\n",
    "image = cv2.imread(path_analyze_image)\n",
    "\n",
    "# Extraer los rostros de la imagen a analizar\n",
    "print(\"Extracting faces...\")\n",
    "faces_data = extract_and_expand_faces(path_analyze_image, 0.2)\n",
    "\n",
    "# Obtener la lista de personas de referencia\n",
    "list_dict_people = get_people_list(\"./data/classroom/ref-standard/\")\n",
    "\n",
    "# Iterar sobre la lista de personas para comparar\n",
    "index = 0\n",
    "image_with_verified_face = image.copy()\n",
    "\n",
    "print(\"Comparing faces...\")\n",
    "\n",
    "for dict_person in list_dict_people:\n",
    "    print(f\"Person: {index + 1}/{len(list_dict_people)}\")\n",
    "    person_name = dict_person[\"name\"]\n",
    "    ref_image_paths = dict_person[\"images_paths\"]\n",
    "\n",
    "    # Establecer el número máximo de rostros de referencia\n",
    "    max_ref_faces = 3  # Establece el número máximo de rostros de referencia\n",
    "    limited_ref_faces = []  # Inicializamos una lista para los rostros de referencia limitados\n",
    "\n",
    "    # Extraer rostros de todas las imágenes de referencia para esta persona\n",
    "    ref_faces_list = []\n",
    "    for ref_img_path in ref_image_paths:\n",
    "        ref_faces_list.extend(extract_and_expand_faces(ref_img_path, 0.18))\n",
    "\n",
    "    # Limitar el número de rostros de referencia a max_ref_faces\n",
    "    limited_ref_faces = ref_faces_list[:max_ref_faces]  # Solo tomamos los primeros max_ref_faces\n",
    "\n",
    "    # Gestionar las colisiones de rostros\n",
    "    print(f\"Handling collisions for {person_name}...\")\n",
    "\n",
    "    # Llamamos a la función con los rostros limitados\n",
    "    result_list = handle_face_collisions(\n",
    "        faces_data,  # Rostros detectados en la imagen a analizar\n",
    "        limited_ref_faces,  # Pasamos solo los primeros `max_ref_faces` rostros de referencia\n",
    "        person_name=person_name,\n",
    "        metric_model_name=\"Facenet512\",\n",
    "        metric_distance=\"cosine\"\n",
    "    )\n",
    "\n",
    "    # Añadir recuadros después de procesar todas las verificaciones\n",
    "    print(\"Drawing boxes for verified faces...\"f\"\\n{'.'*50}\")\n",
    "\n",
    "    image_with_verified_face = draw_verified_faces(image_with_verified_face, result_list, name_person=person_name)\n",
    "    index += 1\n",
    "\n",
    "# Convertir BGR a RGB para mostrar con Matplotlib\n",
    "image_rgb = cv2.cvtColor(image_with_verified_face, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Mostrar la imagen\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('on')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301524fc-ff10-48e9-b632-496d84b8cee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
